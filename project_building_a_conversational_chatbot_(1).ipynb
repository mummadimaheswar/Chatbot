{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW2GlcN_Y3LG"
      },
      "source": [
        "# Aim of the Project\n",
        "Aim of the project is to build an intelligent conversational chatbot, Riki, that can understand complex queries from the user and intelligently respond.\n",
        "\n",
        "### Background\n",
        "R-Intelligence Inc., an AI startup, has partnered with an online chat and discussion website bluedit.io. They have an average of over 5 million active customers across the globe and more than 100,000 active chat rooms. Due to the increased traffic, they are looking at improving their user experience with a chatbot moderator, which helps them engage in a meaningful conversation and keeps them updated on trending topics, while merely chatting with Riki, a chatbot. The Artificial Intelligence-powered chat experience provides easy access to information and a host of options to the customers.\n",
        "\n",
        "### Business Requirement\n",
        "R-Intelligence Inc. has invested in Python, PySpark, and Tensorflow. Using emerging technologies of Artificial Intelligence, Machine Learning, and Natural Language Processing, Riki – the chatbot should make the whole conversation as realistic as talking to an actual human.\n",
        "The chatbot should understand that users have different intents and make it extremely simple to work around these by presenting the users with options and recommendations that best suit their needs.\n",
        "### Suggested Approach\n",
        "R-Intelligence Inc. used an approach using only Natural Language Processing, in which Seq2seq models (encoder and Decoder) are used as the state-of-the-art approach to implement end to\n",
        "end text generation for a conversational bot.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "### Tasks to be performed\n",
        "* Download the glove model available at https://nlp.stanford.edu/projects/glove/\n",
        "* Specification: Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n",
        "*   Load the glove word embedding into a dictionary where the key is a unique word token and the value is a d dimension vector\n",
        "*   Data Preparation - Filter the conversations till max word length and convert the dialogues pairs into input text and target texts. Put start and end token to recognize the beginning and end of the sentence token\n",
        "*   Create two dictionaries:\n",
        "    *     target_word2id\n",
        "    *     target_id2word\n",
        "\n",
        "and save it as NumPy file format in the disk.\n",
        "* Prepare the input data with embedding. The input data is a list of lists:\n",
        "    *    First list is a list of sentences\n",
        "    *    Each sentence is a list of words\n",
        "* Generate training data per batch\n",
        "* Define the model architecture and perform the following steps:\n",
        "    *    Step 1: Use a LSTM encoder to get input words encoded in the form of (encoder outputs, encoder hidden state, encoder context) from input words\n",
        "    *    Step 2: Use a LSTM decoder to get target words encoded in the form of (decoder outputs, decoder hidden state, decoder context) from target words. Use encoder hidden states and encoder context (represents input memory) as initial state.\n",
        "    *    Step 3: Use a dense layer to predict the next token out of the vocabulary given decoder output generated by Step 2.\n",
        "    *    Step 4: Use loss ='categorical_crossentropy' and optimizer='rmsprop'\n",
        "* Generate the model summary\n",
        "* Finally generate the prediction\n",
        "\n",
        "### Dataset Description\n",
        "Dataset: Cornell Movie Dialogue corpus\n",
        "#### Brief Description\n",
        "This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:\n",
        "* 220,579 conversational exchanges between 10,292 pairs of movie characters\n",
        "* involves 9,035 characters from 617 movies\n",
        "* in total 304,713 utterances\n",
        "* movie metadata included:\n",
        "    * genres\n",
        "    * release year\n",
        "    * IMDB rating\n",
        "    * number of IMDB votes\n",
        "    * IMDB rating\n",
        "* character metadata included:\n",
        "    * - gender (for 3,774 characters)\n",
        "    * - position on movie credits (3,321 characters)\n",
        "\n",
        "#### File Description\n",
        "In all files the field separator is \" +++$+++ \"\n",
        "* movie_titles_metadata.txt\n",
        "Contains information about each movie title\n",
        "    * fields:\n",
        "        * movieID,\n",
        "        * movie title,\n",
        "        * movie year,\n",
        "        * IMDB rating,\n",
        "        * no. IMDB votes,\n",
        "        * genres in the format ['genre1','genre2',É,'genreN']\n",
        "* movie_characters_metadata.txt\n",
        "Contains information about each movie character\n",
        "    * fields:\n",
        "        * characterID\n",
        "        * character name\n",
        "        * movieID\n",
        "        * movie title\n",
        "        * gender (\"?\" for unlabeled cases)\n",
        "        * position in credits (\"?\" for unlabeled cases)\n",
        "* movie_lines.txt\n",
        "Contains the actual text of each utterance\n",
        "    * fields:\n",
        "        * lineID\n",
        "        * characterID (who uttered this phrase)\n",
        "        * movieID\n",
        "        * character name\n",
        "        * text of the utterance\n",
        "* movie_conversations.txt\n",
        "Contains the structure of the conversations\n",
        "    * fields\n",
        "        * characterID of the first character involved in the conversation\n",
        "        * characterID of the second character involved in the conversation\n",
        "        * movieID of the movie in which the conversation occurred\n",
        "        * List of the utterances that make the conversation, in chronological\n",
        "            order: ['lineID1','lineID2',É,'lineIDN'] has to be matched with movie_lines.txt to reconstruct the actual content\n",
        "* raw_script_urls.txt\n",
        "Contains the urls from which the raw sources were retrieved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yhaweC1OY3LI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate, TimeDistributed\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65d8v8NhZhhO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqyrItr8Y3LJ"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "yudEXDDCY3LJ"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "lines = open('/content/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open('/content/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5p8j1LXY3LK",
        "outputId": "b25a09d2-6422-4919-a874-e2ea6d9bc341"
      },
      "outputs": [],
      "source": [
        "# The sentences that we will be using to train our model.\n",
        "lines[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQGYzEHCY3LK",
        "outputId": "f936cb5e-0332-48eb-984f-2af3f166ecb7"
      },
      "outputs": [],
      "source": [
        "# The sentences' ids, which will be processed to become our input and target data.\n",
        "conv_lines[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSwmy2lrY3LK"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY663ytyY3LK"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to map each line's id with its text\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "#     print(_line)\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BRBlh5TY3LL"
      },
      "outputs": [],
      "source": [
        "# Create a list of all of the conversations' lines' ids.\n",
        "convs = []\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "#     print(_line)\n",
        "    convs.append(_line.split(','))\n",
        "\n",
        "#id and conversation sample\n",
        "# for k in convs[300]:\n",
        "#     print (k, id2line[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UX6wotKY3LL"
      },
      "source": [
        "### Creating question inputs and answer targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iNxR38aY3LL",
        "outputId": "ebbbb030-e499-43b7-e702-50f60cbb0aa9"
      },
      "outputs": [],
      "source": [
        "# Sort the sentences into questions (inputs) and answers (targets)\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        questions.append(id2line[conv[i]])\n",
        "        answers.append(id2line[conv[i+1]])\n",
        "\n",
        "# Compare lengths of questions and answers\n",
        "print(len(questions))\n",
        "print(len(answers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha6aISg2Y3LL"
      },
      "source": [
        "### Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u57xMqFIY3LL"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLdzxE9nY3LL"
      },
      "outputs": [],
      "source": [
        "# Clean the data\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4rR6aCOY3LL",
        "outputId": "37052431-c42d-4e5d-d6fb-b6c26dd71908"
      },
      "outputs": [],
      "source": [
        "print(\"======Original questions and answers=======\")\n",
        "print(questions[2])\n",
        "print(answers[2])\n",
        "\n",
        "print(\"\\n======Cleaned questions and answers===\")\n",
        "print(clean_questions[2])\n",
        "print(clean_answers[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIrwKFYPY3LM"
      },
      "source": [
        "### Selecting questions and answers with appropriate length (<20 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-UsnZGpY3LM",
        "outputId": "822ef014-2daa-444c-d71e-bee1f2889d69"
      },
      "outputs": [],
      "source": [
        "# Find the length of sentences\n",
        "lengths = []\n",
        "for question in clean_questions:\n",
        "    lengths.append(len(question.split()))\n",
        "for answer in clean_answers:\n",
        "    lengths.append(len(answer.split()))\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "\n",
        "print(lengths['counts'].describe())\n",
        "\n",
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))\n",
        "print(np.percentile(lengths, 99))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbpQ6G31Y3LM"
      },
      "source": [
        "Let's choose length of sequences to be of maximum 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW7WcInNY3LM"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 20000 #50000#15000 #14999 #to decide the dimension of sentence’s one-hot vector\n",
        "EMBEDDING_DIM = 100 #to decide the dimension of Word2Vec\n",
        "MAX_LEN = 20  #to unify the length of the input sentences\n",
        "NUM_SAMPLES = 60000 #60000  # Number of samples to train on.\n",
        "GLOVE_DIR = '/kaggle/input/glove-global-vectors-for-word-representation'\n",
        "EPOCHS=40 #10\n",
        "BATCH_SIZE=256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80xcyIOAY3LM",
        "outputId": "0af33ec7-c9b7-42dd-f417-d96040d2e021"
      },
      "outputs": [],
      "source": [
        "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "# Filter out the questions that are too short/long\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i, question in enumerate(clean_questions):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(clean_answers[i])\n",
        "\n",
        "# Filter out the answers that are too short/long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(short_answers_temp):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "\n",
        "print(len(short_questions))\n",
        "print(len(short_answers))\n",
        "\n",
        "\n",
        "r = np.random.randint(1,len(short_questions))\n",
        "for i in range(r, r+3):\n",
        "    print(short_questions[i])\n",
        "    print(short_answers[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ldB-ywjY3LM"
      },
      "outputs": [],
      "source": [
        "#choosing number of samples\n",
        "# encoder_input_text = clean_questions[:NUM_SAMPLES]\n",
        "encoder_input_text = short_questions[:NUM_SAMPLES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_plkEqyIY3LM"
      },
      "source": [
        "### Put BOS tag and EOS tag for decoder input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV_1p1p3Y3LM",
        "outputId": "41003abf-41a3-4bf2-cef8-43e7f9c1dd73"
      },
      "outputs": [],
      "source": [
        "def tagger(input_text):\n",
        "  bos = \"<BOS> \"\n",
        "  eos = \" <EOS>\"\n",
        "  final_target = [bos + text + eos for text in input_text]\n",
        "  return final_target\n",
        "\n",
        "# clean_answers = clean_answers[:NUM_SAMPLES]\n",
        "# decoder_input_text = tagger(clean_answers)\n",
        "# decoder_input_text[:5]\n",
        "short_answers = short_answers[:NUM_SAMPLES]\n",
        "decoder_input_text = tagger(short_answers)\n",
        "decoder_input_text[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqUWcrPQY3LM",
        "outputId": "5ed5702e-0fea-4795-9e76-c1d41ea692cf"
      },
      "outputs": [],
      "source": [
        "print(len(encoder_input_text))\n",
        "print(len(decoder_input_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YOR0zntY3LM"
      },
      "source": [
        "### Make Vocabulary (VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncEkCQSWY3LM",
        "outputId": "57ce9809-96ca-4b8d-8750-237d231cfc6d"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words= VOCAB_SIZE, filters='')\n",
        "\n",
        "def vocab_creater(text_lists, VOCAB_SIZE):\n",
        "  tokenizer.fit_on_texts(text_lists)\n",
        "  dictionary = tokenizer.word_index\n",
        "\n",
        "  word2idx = {}\n",
        "  idx2word = {}\n",
        "  for k, v in dictionary.items():\n",
        "      if v < VOCAB_SIZE:\n",
        "          word2idx[k] = v\n",
        "          idx2word[v] = k\n",
        "      if v >= VOCAB_SIZE-1:\n",
        "          continue\n",
        "\n",
        "  return word2idx, idx2word\n",
        "\n",
        "word2idx, idx2word = vocab_creater(text_lists=encoder_input_text+decoder_input_text, VOCAB_SIZE=VOCAB_SIZE)\n",
        "\n",
        "#print first few key/value pairs\n",
        "word2idx_first5pairs = {k: word2idx[k] for k in list(word2idx)[:5]}\n",
        "print('word2idx: ', word2idx_first5pairs)\n",
        "\n",
        "idx2word_first5pairs = {k: idx2word[k] for k in list(idx2word)[:5]}\n",
        "print('\\nidx2word: ', idx2word_first5pairs)\n",
        "\n",
        "# Check the length of the dictionaries.\n",
        "print('word2idx length', len(word2idx))\n",
        "print('idx2word length', len(idx2word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftq24G5wY3LM"
      },
      "source": [
        "### Tokenize Bag of words to Bag of IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_3QtVqCY3LM",
        "outputId": "7c1e8f27-03ce-4b48-ea66-093d36724c2d"
      },
      "outputs": [],
      "source": [
        "def text2seq(tokenizer, encoder_text, decoder_text, VOCAB_SIZE):\n",
        "\n",
        "#   tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "  encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
        "  decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
        "\n",
        "  return encoder_sequences, decoder_sequences\n",
        "\n",
        "encoder_sequences, decoder_sequences = text2seq(tokenizer, encoder_input_text, decoder_input_text, VOCAB_SIZE)\n",
        "print('encoder_sequences:\\n', encoder_sequences[:5])\n",
        "print('\\ndecoder_sequences:\\n', decoder_sequences[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F09Evvw-Y3LM"
      },
      "source": [
        "### Padding (MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ARJYxQHY3LN",
        "outputId": "659f916f-8876-4d99-843a-8ed079fa84d1"
      },
      "outputs": [],
      "source": [
        "def padding(encoder_sequences, decoder_sequences, MAX_LEN):\n",
        "\n",
        "  encoder_input_data = pad_sequences(encoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
        "  decoder_input_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
        "\n",
        "  return encoder_input_data, decoder_input_data\n",
        "\n",
        "encoder_input_data, decoder_input_data = padding(encoder_sequences, decoder_sequences, MAX_LEN)\n",
        "print('encoder_input_data:\\n', encoder_input_data)\n",
        "print('\\ndecoder_input_data:\\n', decoder_input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBY2zOXDY3LN"
      },
      "source": [
        "### Reshape the Data to neural network shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rwx4TzQY3LN"
      },
      "outputs": [],
      "source": [
        "num_samples = len(encoder_sequences)\n",
        "\n",
        "def decoder_output_creator(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE):\n",
        "  decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"float32\")\n",
        "  for i, seqs in enumerate(decoder_input_data):\n",
        "      for t, seq in enumerate(seqs):\n",
        "          if t > 0:\n",
        "                decoder_output_data[i][t][seq] = 1.  #decoder_output_data[i, t, seq] = 1.\n",
        "\n",
        "  return decoder_output_data\n",
        "\n",
        "decoder_output_data = decoder_output_creator(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mZdkpFhY3LN",
        "outputId": "1070d70b-639c-4229-f23d-c095bf759ba3"
      },
      "outputs": [],
      "source": [
        "print (encoder_input_data.shape)\n",
        "print (decoder_input_data.shape)\n",
        "print (decoder_output_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T05-x4VSeJUZ",
        "outputId": "0363b878-8c41-4df4-cfe5-4dd71c96ba37"
      },
      "outputs": [],
      "source": [
        "# Download the glove model\n",
        "!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip -q glove.twitter.27B.zip\n",
        "\n",
        "# Update GLOVE_DIR to the correct path\n",
        "GLOVE_DIR = '/content/'\n",
        "print(f\"GLOVE_DIR is now set to: {GLOVE_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9mnRdlZY3LN"
      },
      "source": [
        "### Word Embedding (EMBEDDING_DIM)\n",
        "\n",
        "We use Pretraind Word2Vec Model from Glove. We can create embedding layer with Glove with 3 steps:\n",
        "1. Call Glove file\n",
        "1. Create Embedding Matrix from our Vocabulary\n",
        "1. Create Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR_7PiseY3LN",
        "outputId": "26e88b79-aa03-4ad5-c8d0-19106a54e3bd"
      },
      "outputs": [],
      "source": [
        "# Call Glove file\n",
        "def glove_100d_dictionary(glove_dir):\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(glove_dir, 'glove.twitter.27B.100d.txt'))\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index = glove_100d_dictionary(GLOVE_DIR)\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCM7864SY3LN",
        "outputId": "6dd8a20c-8ce4-48ca-ecc5-883ada882e35"
      },
      "outputs": [],
      "source": [
        "#Create Embedding Matrix from our Vocabulary\n",
        "def embedding_matrix_creater(max_words, embedding_dimension):\n",
        "  embedding_matrix = np.zeros((max_words, embedding_dimension))  #np.zeros((len(word2idx) + 1, embedding_dimension))\n",
        "  for word, i in word2idx.items():\n",
        "        if i < max_words:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix\n",
        "\n",
        "embedding_matrix = embedding_matrix_creater(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "print(embedding_matrix.shape)\n",
        "print((len(word2idx) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do5NOcjUY3LN",
        "outputId": "023ad035-a6d8-4211-855c-e594d79d2001"
      },
      "outputs": [],
      "source": [
        "#Create Embedding Layer\n",
        "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
        "\n",
        "  embedding_layer = Embedding(input_dim = VOCAB_SIZE,\n",
        "                              output_dim = EMBEDDING_DIM,\n",
        "                              input_length = MAX_LEN,\n",
        "                              weights = [embedding_matrix],\n",
        "                              trainable = False)\n",
        "  return embedding_layer\n",
        "\n",
        "embedding_layer = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix)\n",
        "\n",
        "embedding_layer2 = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, None, embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5OLboF3Y3LN"
      },
      "source": [
        "###  Split data for training validation & testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9riZAkBAY3LN"
      },
      "outputs": [],
      "source": [
        "def data_spliter(encoder_input_data, decoder_input_data, test_size1=0.2, test_size2=0.3):\n",
        "  en_train, en_test, de_train, de_test = train_test_split(encoder_input_data, decoder_input_data, test_size=test_size1)\n",
        "  en_train, en_val, de_train, de_val = train_test_split(en_train, de_train, test_size=test_size2)\n",
        "\n",
        "  return en_train, en_val, en_test, de_train, de_val, de_test\n",
        "\n",
        "en_train, en_val, en_test, de_train, de_val, de_test = data_spliter(encoder_input_data, decoder_input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8WUUcApY3LS"
      },
      "source": [
        "# Create Seq2Seq Neural Network Architecture\n",
        "\n",
        "Seq2Seq is a type of Encoder-Decoder model using RNN. It can be used as a model for machine interaction and machine translation. By learning a large number of sequence pairs, this model generates one from the other. More kindly explained, the definition of Seq2Seq is below:\n",
        "* Input: Text Data\n",
        "* Output: Text Data as well\n",
        "\n",
        "Below is our Seq2Seq Neural Network Architecture using LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "wrHF1vmfY3LS",
        "outputId": "7ec6e1c0-1615-4ece-8502-f95fcee81422"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate, TimeDistributed\n",
        "\n",
        "def build_seq2seq_model(HIDDEN_DIM=300):\n",
        "    #set up the encoder\n",
        "    encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
        "    encoder_embedding = embedding_layer(encoder_inputs)\n",
        "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
        "    decoder_embedding = embedding_layer(decoder_inputs)\n",
        "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
        "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "    outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
        "\n",
        "    #create seq2seq model\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) #sparse_categorical_crossentropy as labels in a single integer array\n",
        "\n",
        "    #create encoder model\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    #Create sampling/decoder model\n",
        "    decoder_state_input_h  = Input(shape=(HIDDEN_DIM,))\n",
        "    decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_embedding2= embedding_layer(decoder_inputs)\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_embedding2, initial_state=decoder_states_inputs)\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n",
        "\n",
        "def build_seq2seq_model2(HIDDEN_DIM=300):\n",
        "    #set up the encoder\n",
        "    encoder_inputs = Input(shape=(None, ), dtype='int32',)\n",
        "    encoder_embedding = embedding_layer2(encoder_inputs)\n",
        "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(None, ), dtype='int32',)\n",
        "    decoder_embedding = embedding_layer2(decoder_inputs)\n",
        "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
        "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "    outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
        "\n",
        "    #create seq2seq model\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) #sparse_categorical_crossentropy as labels in a single integer array\n",
        "\n",
        "    #create encoder model\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    #Create sampling/decoder model\n",
        "    decoder_state_input_h  = Input(shape=(HIDDEN_DIM,))\n",
        "    decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_embedding2= embedding_layer2(decoder_inputs)\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_embedding2, initial_state=decoder_states_inputs)\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n",
        "\n",
        "model, encoder_model, decoder_model = build_seq2seq_model2(HIDDEN_DIM=300)\n",
        "model.summary()\n",
        "\n",
        "# model, encoder_model, decoder_model = build_seq2seq_model(HIDDEN_DIM=300)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "wwDthpuJY3LS",
        "outputId": "7429d574-45c2-42f8-f67d-2fd3ebd026d0"
      },
      "outputs": [],
      "source": [
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "J2UxkrF4Y3LS",
        "outputId": "0714bfef-a005-48fb-a781-447abe965eef"
      },
      "outputs": [],
      "source": [
        "decoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C10EOP95Y3LS"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IMPy0ctTY3LS",
        "outputId": "eb5287ef-c0fe-4432-fc2b-a2478da2b811"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "#early stopping & saving\n",
        "# es = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
        "mcp = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          validation_split=0.05,\n",
        "          callbacks= [mcp]\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIBn9_qSY3LS"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8dfx5zfY3LS"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = word2idx['<bos>']\n",
        "    eos = word2idx['<eos>']\n",
        "    output_sentence = []\n",
        "\n",
        "    for _ in range(MAX_LEN):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        # Sample a token\n",
        "        idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        word = ''\n",
        "\n",
        "        if idx > 0:\n",
        "            word = idx2word[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        target_seq[0, 0] = idx\n",
        "        states_value = [h, c]  # Update states\n",
        "\n",
        "    return ' '.join(output_sentence)\n",
        "\n",
        "for index in range(10):\n",
        "    i = np.random.randint(1, len(encoder_input_data))\n",
        "    input_seq = encoder_input_data[i:i+1]\n",
        "    translation = translate_sentence(input_seq)\n",
        "    print('-')\n",
        "    print('Input:', short_answers[i])\n",
        "    print('Response:', translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ef0262d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
